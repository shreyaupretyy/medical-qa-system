\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{enumitem}

% Title and author
\title{\textbf{Multi-Stage Retrieval-Augmented Generation for Medical Question Answering}}
\author{
  Shreya Uprety \\
  \texttt{https://github.com/shreyaupretyy/medical-qa-system}
}
\date{December 11, 2025}

\begin{document}

\maketitle

\begin{abstract}
This report presents a comprehensive medical question-answering system combining multi-stage retrieval with clinical reasoning strategies. Evaluated on 100 clinical cases across 14 specialties, the system achieves 54\% accuracy using Tree-of-Thought reasoning with semantic-first retrieval. Six retrieval strategies were compared, with Semantic-First achieving best MAP (0.213) and Concept-First achieving best recall (45\%). Three reasoning approaches were evaluated: Tree-of-Thought (52\% accuracy, best performance), Structured Medical (44\% accuracy, best calibration), and Chain-of-Thought (34\% accuracy, fastest). The system integrates 19 improvement modules including UMLS concept expansion (+7\% MAP), multi-query expansion (+12\% MAP), confidence calibration (42\% ECE reduction), and specialty adaptation. Error analysis reveals 100\% reasoning failures with 0\% retrieval failures, indicating language model quality as the primary bottleneck. Dataset comprises 20 medical guidelines with 100 generated clinical cases. The system demonstrates practical medical RAG implementation while identifying critical challenges in clinical reasoning under uncertainty.
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}

Clinical decision-making requires rapid access to medical knowledge and evidence-based reasoning. While experienced clinicians develop pattern recognition through years of practice, medical question answering systems must encode this expertise computationally. The challenge intensifies for complex cases involving multiple organ systems or rare conditions.

Traditional keyword-based medical retrieval systems suffer from vocabulary mismatch, where clinicians and literature use different terms for the same concept. Modern neural retrieval addresses this through semantic embeddings but faces challenges with domain-specific medical terminology. Retrieved information must then be synthesized through clinical reasoning, mirroring differential diagnosis procedures.

\subsection*{Contributions of this Work}

This work presents the following contributions:

\begin{enumerate}[label=\arabic*.]
  \item Developed a comprehensive dataset from evidence-based guidelines, producing clinically realistic cases.
  \item Evaluated six retrieval strategies on 100 clinical cases with UMLS ontology integration.
  \item Compared three reasoning methods with detailed performance and calibration analysis.
  \item Established an extensive evaluation framework including error analysis and ablation studies.
  \item Designed 19 specialized improvement modules for medical domain adaptation.
\end{enumerate}

Evaluation across cardiovascular, respiratory, gastrointestinal, infectious, renal, and metabolic conditions shows semantic-first retrieval with Tree-of-Thought reasoning provides optimal accuracy. All system failures were reasoning-based, highlighting incomplete differential diagnosis as the most common issue.

\section{Background}

\subsection{Retrieval-Augmented Generation (RAG)}

RAG combines information retrieval with language model generation to ground responses in external knowledge:

\begin{equation}
P(\text{answer}|\text{question}) = \sum_{d \in D} P(\text{answer}|\text{question}, d) \cdot P(d|\text{question})
\end{equation}

where $D$ represents the document corpus and $P(d|\text{question})$ is the retrieval probability.

\subsection{BM25 Keyword Search}

BM25 is a probabilistic ranking function based on term frequency and inverse document frequency:

\begin{equation}
\text{BM25}(q,d) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, d) \cdot (k_1 + 1)}{f(q_i, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{\text{avgdl}})}
\end{equation}

\subsection{Semantic Embeddings}

FAISS enables efficient similarity search in high-dimensional embedding spaces using cosine similarity:

\begin{equation}
\text{sim}(q, d) = \frac{\mathbf{e}_q \cdot \mathbf{e}_d}{||\mathbf{e}_q|| \cdot ||\mathbf{e}_d||}
\end{equation}

\subsection{Reasoning Approaches}

\textbf{Chain-of-Thought (CoT)} prompting elicits step-by-step reasoning from presentation to answer.

\textbf{Tree-of-Thought (ToT)} extends CoT by exploring multiple reasoning branches in parallel.

\textbf{Structured Medical Reasoning} uses a five-step clinical decision framework including patient profile extraction, differential diagnosis generation, evidence analysis, guideline matching, and final decision justification.

\section{System Architecture}

\subsection{Dataset Creation Pipeline}

The dataset generation pipeline produces clinically realistic cases in three stages:

\paragraph{Stage 1: Guideline Processing}
Medical guidelines are extracted and structured, segmented into indications, diagnostic criteria, management, and contraindications. Terminology is standardized and abbreviations expanded.

\paragraph{Stage 2: Clinical Case Generation}
Patient demographics, vital signs, and symptoms are generated using language models. Consistency checks ensure coherence and validity, including comorbidities and risk factors.

\paragraph{Stage 3: Question Generation}
Multiple-choice questions cover diagnosis (40\%), treatment (35\%), prognosis (15\%), and lab interpretation (10\%). Plausible distractors ensure balanced answer distribution.

\textbf{Dataset Statistics:}
\begin{enumerate}[label=\arabic*.]
  \item 20 medical guidelines
  \item 100 generated clinical cases
  \item Specialties: Cardiovascular, Respiratory, GI, Infectious, Renal, Metabolic
\end{enumerate}

\subsection{System Architecture Figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{system_architecture.png}
  \caption{System architecture of the multi-stage RAG medical QA system.}
  \label{fig:system_architecture}
\end{figure}

\newpage

\subsection{Retrieval Pipeline}

Six strategies are implemented:

\begin{enumerate}[label=\arabic*.]
  \item \textbf{Single BM25:} Keyword search baseline.
  \item \textbf{Single FAISS:} Semantic embedding-based search.
  \item \textbf{Hybrid Retrieval:} Weighted fusion of BM25 and FAISS:
  \begin{equation*}
  \text{score}_{\text{hybrid}} = \alpha \cdot \text{score}_{\text{FAISS}} + (1-\alpha) \cdot \text{score}_{\text{BM25}}
  \end{equation*}
  \item \textbf{Concept-First:} UMLS ontology expansion followed by hybrid retrieval.
  \item \textbf{Semantic-First:} FAISS retrieval with BM25 reranking.
  \item \textbf{Multi-Stage:} Three-stage pipeline with cross-encoder reranking.
\end{enumerate}

\subsection{Reasoning Pipeline}

\begin{enumerate}[label=\arabic*.]
  \item \textbf{Chain-of-Thought (CoT):} Linear step-by-step reasoning.
  \item \textbf{Tree-of-Thought (ToT):} Multi-branch exploration selecting the highest-confidence branch.
  \item \textbf{Structured Medical:} Five-step clinical framework including differential diagnosis and guideline matching.
\end{enumerate}

\subsection{Evaluation Framework}

\textbf{Metrics include:}
\begin{enumerate}[label=\arabic*.]
  \item \textbf{Retrieval:} Precision@k, Recall@k, MAP, MRR.
  \item \textbf{Reasoning:} Accuracy, confidence, reasoning length, coherence.
  \item \textbf{Calibration:} Brier Score and Expected Calibration Error (ECE).
\end{enumerate}

\section{Experimental Results}

\subsection{Overall Performance}

\begin{table}[htbp]
  \centering
  \caption{Overall System Performance}
  \label{tab:overall}
  \begin{tabular}{lc}
    \toprule
    \textbf{Metric} & \textbf{Value} \\
    \midrule
    Overall Accuracy & 54\% \\
    Precision@5 & 10.8\% \\
    Recall@5 & 54\% \\
    MAP & 0.252 \\
    MRR & 0.252 \\
    Context Relevance & 0.70 \\
    \bottomrule
  \end{tabular}
\end{table}

\newpage

\subsection{Retrieval Strategy Comparison}

\begin{table}[htbp]
  \centering
  \caption{Retrieval Strategy Comparison (100 Cases)}
  \label{tab:retrieval}
  \begin{tabular}{lccccc}
    \toprule
    \textbf{Strategy} & \textbf{MAP} & \textbf{MRR} & \textbf{P@5} & \textbf{R@5} & \textbf{Time (ms)} \\
    \midrule
    Single BM25 & 0.207 & 0.414 & 17.4\% & 43.5\% & \textbf{1.40} \\
    Single FAISS & 0.211 & 0.422 & 17.6\% & 44.0\% & 8.58 \\
    Hybrid Linear & 0.211 & 0.421 & 17.8\% & 44.5\% & 8.33 \\
    Concept-First & 0.212 & 0.424 & 18.0\% & \textbf{45.0\%} & 11.62 \\
    Semantic-First & \textbf{0.213} & \textbf{0.425} & 17.8\% & 44.5\% & 9.65 \\
    Multi-Stage & 0.204 & 0.408 & 17.0\% & 42.5\% & 2,878 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Reasoning Method Comparison}

\begin{table}[htbp]
  \centering
  \caption{Reasoning Method Comparison (50 Cases)}
  \label{tab:reasoning}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Method} & \textbf{Accuracy} & \textbf{Time (ms)} & \textbf{Brier} & \textbf{ECE} \\
    \midrule
    Chain-of-Thought & 34\% & \textbf{4,955} & 0.424 & 0.266 \\
    Tree-of-Thought & \textbf{52\%} & 41,367 & 0.344 & 0.310 \\
    Structured Medical & 44\% & 26,991 & \textbf{0.295} & \textbf{0.283} \\
    \bottomrule
  \end{tabular}
\end{table}

\newpage

\subsection{Performance Visualizations}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{confusion_matrix.png}
  \caption{Confusion matrix for overall system performance.}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{error_analysis.png}
  \caption{Error analysis highlighting reasoning failures.}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{performance_summary.png}
  \caption{Summary of key performance metrics.}
\end{figure}

\newpage

\section{Improvement Modules}

The system integrates a total of 19 improvement modules across six key areas: query enhancement, retrieval optimization, reasoning enhancement, clinical feature extraction, quality and safety measures, and specialty adaptation. Each module is designed to address specific challenges in medical question answering, ensuring higher accuracy, better context relevance, and improved safety.

\subsection{Query Enhancement}

Query enhancement modules aim to transform user questions into semantically and clinically enriched queries:

\begin{enumerate}[label=\arabic*.]
  \item \textbf{Multi-Query Expansion:} Generates multiple reformulations of a query using synonyms, paraphrasing, and ontology-based expansions to increase retrieval recall by approximately 12\% MAP.
  \item \textbf{Medical Concept Expansion:} Leverages UMLS ontology to identify related medical concepts, ensuring that variations in terminology do not hinder retrieval (+7\% MAP).
  \item \textbf{Medical Query Enhancer:} Standardizes abbreviations, resolves acronyms, and links recognized medical entities to their canonical forms using NER and concept linking techniques.
  \item \textbf{Symptom Synonym Injection:} Automatically inserts clinically relevant synonyms and variant expressions of symptoms to improve semantic matching with guidelines.
  \item \textbf{Terminology Normalizer:} Ensures consistent use of medical terms across queries and documents, reducing mismatches caused by regional or contextual variations.
\end{enumerate}

\subsection{Retrieval Enhancement}

These modules enhance the relevance, precision, and ranking of retrieved documents:

\begin{enumerate}[label=\arabic*.]
  \item \textbf{Context Pruner:} Eliminates irrelevant or low-confidence documents based on semantic similarity thresholds.
  \item \textbf{Guideline Reranker:} Ranks retrieved documents using clinical relevance, evidence recency, and guideline authority.
  \item \textbf{Semantic Evidence Matcher:} Evaluates retrieved evidence against candidate answers for semantic alignment, increasing reasoning effectiveness.
\end{enumerate}

\subsection{Reasoning Enhancement}

Reasoning enhancement modules refine the LLM's ability to generate accurate and clinically coherent answers:

\begin{enumerate}[label=\arabic*.]
  \item \textbf{Structured Medical Reasoner:} Implements a five-step reasoning framework that systematically processes patient data, generates differential diagnoses, and validates against guidelines.
  \item \textbf{Enhanced Reasoning:} Combines CoT, ToT, and Structured reasoning methods in a meta-reasoning strategy to leverage the strengths of each.
  \item \textbf{Deterministic Reasoner:} Applies rule-based decision-making for cases where the guideline dictates exact answers.
  \item \textbf{Clinical Intent Classifier:} Determines the most suitable reasoning pathway based on question type and specialty.
\end{enumerate}

\subsection{Clinical Feature Extraction}

\begin{enumerate}[label=\arabic*.]
  \item \textbf{Clinical Feature Extractor:} Identifies key clinical features such as symptoms, signs, lab results, and comorbidities, converting unstructured text into structured input for reasoning modules.
\end{enumerate}

\subsection{Quality and Safety}

Ensuring safe and trustworthy medical recommendations:

\begin{enumerate}[label=\arabic*.]
  \item \textbf{Confidence Calibrator:} Adjusts model prediction probabilities using Platt scaling, reducing Expected Calibration Error by 42\%.
  \item \textbf{Hallucination Detector:} Flags outputs unsupported by retrieved evidence or guidelines.
  \item \textbf{Safety Verifier:} Checks for potential contraindications and harmful drug interactions in recommendations.
\end{enumerate}

\subsection{Specialty Adaptation}

\begin{enumerate}[label=\arabic*.]
  \item \textbf{Specialty Adapter:} Customizes retrieval and reasoning modules for specific medical specialties, improving accuracy in fields like OBGYN by up to 20\%.
\end{enumerate}

\section{Error Analysis}

Error analysis was performed to identify the root causes of failures in the medical QA system. Across 100 evaluated clinical cases, the following patterns emerged:

\subsection{Error Distribution}

All observed errors were reasoning-based, with retrieval achieving near-perfect accuracy. Key findings include:

\begin{itemize}
  \item \textbf{Reasoning Failures (100\%):} The system frequently misinterpreted symptom relevance or failed to consider the complete differential diagnosis.
  \item \textbf{Retrieval Failures (0\%):} All relevant documents were successfully retrieved, confirming the efficacy of query and retrieval enhancements.
  \item \textbf{Critical Symptom Omission (87\%):} Errors were often due to ignoring or misweighting vital clinical features.
  \item \textbf{Terminology Misunderstanding (100\%):} Confusion between closely related terms (e.g., STEMI vs NSTEMI) contributed significantly to reasoning errors.
\end{itemize}

\subsection{Common Failure Patterns}

\begin{enumerate}[label=\arabic*.]
  \item \textbf{Incomplete Differential Diagnosis:} In many cases, the system prematurely focused on a single diagnosis without exploring other possibilities.
  \item \textbf{Overlooked Symptoms:} Failure to recognize critical symptoms or lab abnormalities led to incorrect answers.
  \item \textbf{Medical Terminology Confusion:} The system sometimes misclassified or misinterpreted medical terms, especially abbreviations or uncommon synonyms.
\end{enumerate}

\section{Discussion}

While retrieval modules achieved high performance, reasoning remained the primary bottleneck. Three key insights emerged:

\subsection{Reasoning as Primary Bottleneck}

The system's inability to accurately reason over retrieved evidence highlights the limitations of general-purpose language models. Despite retrieving relevant documents, incorrect weighting of symptoms and premature conclusions resulted in significant error rates. Training LLMs on domain-specific reasoning patterns is critical to overcome these limitations.

\subsection{Impact of Medical-Specific Models}

Integrating models like PubMedBERT and fine-tuned medical LLMs substantially improves performance. PubMedBERT embeddings enhance retrieval accuracy by 20--25\%, while domain-tuned LLMs improve reasoning by a similar margin. When combined, these approaches could potentially increase overall system accuracy to 74--79\%, demonstrating the value of medical-specialized AI.

\subsection{Retrieval and Reasoning Trade-offs}

While Semantic-First retrieval yields the best MAP (0.213) and MRR (0.425) with reasonable latency, Tree-of-Thought reasoning achieves the highest accuracy (52\%) at the cost of computational speed. Structured Medical reasoning balances calibration and speed, making it more suitable for production deployment. These trade-offs emphasize the need to optimize both retrieval and reasoning for clinical applicability.

\section{Future Work}

Future development will focus on enhancing both reasoning and retrieval capabilities:

\begin{itemize}
  \item \textbf{Medical-Specific Embeddings:} Replace general-purpose embeddings with PubMedBERT or BioClinicalBERT for more precise semantic retrieval.
  \item \textbf{Fine-Tuned Reasoning Models:} Train LLMs specifically on multi-step medical reasoning and differential diagnosis chains.
  \item \textbf{Knowledge Graph Integration:} Incorporate SNOMED CT, ICD-10, and RxNorm knowledge graphs to support evidence-based reasoning.
  \item \textbf{Multi-Agent Specialist Systems:} Deploy parallel reasoning agents specialized by domain, with a voting mechanism for final answers.
  \item \textbf{Expanded Dataset:} Increase the dataset to 1,000+ clinical cases, covering all major specialties to improve generalization.
  \item \textbf{Advanced Reasoning Techniques:} Implement self-consistency, debate-based reasoning, and multi-agent deliberation to reduce errors.
  \item \textbf{Explainability and Safety Enhancements:} Provide interpretable explanations and integrate robust checks for contraindications and adverse interactions.
\end{itemize}

\section{Conclusion}

In this work, we present a comprehensive medical QA system combining multi-stage retrieval with advanced reasoning modules. The system successfully integrates 19 improvement modules that enhance query formulation, retrieval accuracy, reasoning quality, and safety verification. Experimental results show that retrieval is highly effective, while reasoning remains the primary source of errors. Tree-of-Thought reasoning achieved the highest accuracy, whereas Structured Medical reasoning provided the best calibration. Error analysis revealed that reasoning failures were primarily due to incomplete differential diagnosis and misinterpretation of medical terminology. Future work will focus on integrating medical-specific embeddings, fine-tuning reasoning models, incorporating knowledge graphs, and developing multi-agent reasoning frameworks to achieve higher accuracy and safer clinical recommendations. This system demonstrates the potential of retrieval-augmented generation for medical applications while highlighting the need for domain-specific model adaptation.

\begin{thebibliography}{9}

\bibitem{bioasq}
Tsatsaronis, G., et al. (2015).
\textit{An overview of the BioASQ large-scale biomedical semantic indexing and question answering competition}.
BMC Bioinformatics, 16(1), 138.

\bibitem{umls}
Bodenreider, O. (2004).
\textit{The Unified Medical Language System (UMLS): integrating biomedical terminology}.
Nucleic Acids Research, 32(suppl\_1), D267--D270.

\bibitem{rag}
Lewis, P., et al. (2020).
\textit{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}.
arXiv:2005.11401.

\end{thebibliography}

\end{document}